{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms,datasets\n",
    "\n",
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Resize((224,224)),transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "train=datasets.ImageFolder('./PetImages/train',transform)\n",
    "test=datasets.ImageFolder('./PetImages/test',transform)\n",
    "trainDataLoader=DataLoader(train,batch_size=64,shuffle=True)\n",
    "testDataLoader=DataLoader(test,batch_size=64,shuffle=False)\n",
    "\n",
    "import torch.nn as nn # AlexNet\n",
    "import torch.nn.functional as F\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #step1\n",
    "        self.Conv_Pool=nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 192, kernel_size=5,stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(192, 384, kernel_size=3,stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3,stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        #step2\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 2),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x=self.Conv_Pool(x)\n",
    "        \n",
    "        x=x.view(-1,256*6*6)\n",
    "        x=self.fc(x)\n",
    "        return x\n",
    "\n",
    "learningRate=0.0001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    1] loss :0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  101] loss :0.693\n",
      "[1,  201] loss :0.693\n",
      "[1,  301] loss :0.633\n",
      "[2,    1] loss :0.683\n",
      "[2,  101] loss :0.587\n",
      "[2,  201] loss :0.666\n",
      "[2,  301] loss :0.555\n",
      "[3,    1] loss :0.508\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\code\\cat vs dog\\main.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/cat%20vs%20dog/main.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()         \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/cat%20vs%20dog/main.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#前向+後向+優化\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/code/cat%20vs%20dog/main.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m outputs \u001b[39m=\u001b[39m net(\u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mto(device))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/cat%20vs%20dog/main.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs,label\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/cat%20vs%20dog/main.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net=CNN()#訓練\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learningRate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net=net.to(device)\n",
    "for epoch in range(10):\n",
    "    for i,(input,label) in enumerate(trainDataLoader,0):#0是下標起始位置預設為0\n",
    "        # data 的格式[inputs, labels]   \n",
    "        #初始為0，清除上個batch的梯度訊息\n",
    "        optimizer.zero_grad()         \n",
    "        #前向+後向+優化\n",
    "        outputs = net(input.to(device))\n",
    "        loss = criterion(outputs,label.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "        if i%100 == 0:\n",
    "            \n",
    "            print('[%d,%5d] loss :%.3f' %\n",
    "                 (epoch+1,i+1,loss.item()))\n",
    "            running_loss = 0.0\n",
    "\n",
    "PATH = './CatVsDog.pth'\n",
    "torch.save(net.state_dict(), PATH)#保存訓練結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the  train images:  98.50185461727443 %\n",
      "Accuracy of the network on the  test images:  90.4047619047619 %\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_net=CNN()#利用train/test data測試訓練成果 \n",
    "PATH = './alexnet0.9.pth'\n",
    "test_net=test_net.to(device)\n",
    "test_net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "total=0\n",
    "correct=0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(trainDataLoader,0):\n",
    "        input,label=data\n",
    "        test_out=test_net(input.to(device))\n",
    "        _,predicted=torch.max(test_out.data,dim=1)\n",
    "        total+=label.size(0)\n",
    "        correct+=(predicted==label.to(device)).sum().item()\n",
    "    print('Accuracy of the network on the  train images: ',float(100*correct/total),'%')\n",
    "\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i,data in enumerate(testDataLoader,0):\n",
    "        input,label=data\n",
    "        test_out=test_net(input.to(device))\n",
    "        _,predicted=torch.max(test_out.data,dim=1)\n",
    "        total+=label.size(0)\n",
    "        correct+=(predicted==label.to(device)).sum().item()\n",
    "    print('Accuracy of the network on the  test images: ',float(100*correct/total),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I guess 0.jpg is a cat.\n",
      "\n",
      "I guess 1.jpg is a cat.\n",
      "\n",
      "I guess 10.jpg is a cat.\n",
      "\n",
      "I guess 2.jpg is a cat.\n",
      "\n",
      "I guess 3.jpg is a cat.\n",
      "\n",
      "I guess 4.jpg is a cat.\n",
      "\n",
      "I guess 5.jpg is a dog.\n",
      "\n",
      "I guess 6.jpg is a dog.\n",
      "\n",
      "I guess 7.jpg is a dog.\n",
      "\n",
      "I guess 8.jpg is a dog.\n",
      "\n",
      "I guess 9.jpg is a dog.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms,datasets\n",
    "import os\n",
    "from PIL import Image     #利用網上圖片測試訓練成果\n",
    "\n",
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Resize((224,224)),transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "for jpg in os.listdir('./test_with_internet_photo'):\n",
    "    img=transform(Image.open('./test_with_internet_photo/{}'.format(jpg)))\n",
    "\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    internet_test_net=CNN()\n",
    "    PATH = './CatVsDog0.9.pth'\n",
    "    internet_test_net=internet_test_net.to(device)\n",
    "    internet_test_net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output=internet_test_net(img.to(device))\n",
    "        _,predicted=torch.max(output.data,dim=1)\n",
    "        if predicted.tolist()==[0]:#0=cat,1=dog\n",
    "            print('I guess {} is a cat.\\n'.format(jpg)) \n",
    "        if predicted.tolist()==[1]:#0=cat,1=dog\n",
    "            print('I guess {} is a dog.\\n'.format(jpg)) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
